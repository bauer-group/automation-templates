name: 'Python Testing Suite'
description: 'Comprehensive Python testing with coverage, reporting, and multiple framework support'
author: 'BAUER GROUP'

inputs:
  test-framework:
    description: 'Test framework (pytest, unittest, nose2, tox)'
    required: false
    default: 'pytest'
  test-path:
    description: 'Path to tests directory or files'
    required: false
    default: 'tests'
  test-pattern:
    description: 'Test file pattern'
    required: false
    default: 'test_*.py'
  test-args:
    description: 'Additional test arguments'
    required: false
    default: ''
  test-command:
    description: 'Custom test command (overrides framework detection)'
    required: false
    default: ''
  working-directory:
    description: 'Working directory for all commands'
    required: false
    default: '.'
  # Coverage Configuration
  collect-coverage:
    description: 'Collect test coverage'
    required: false
    default: 'true'
  coverage-source:
    description: 'Source directories for coverage'
    required: false
    default: '.'
  coverage-format:
    description: 'Coverage report formats (xml,html,term,json)'
    required: false
    default: 'xml,html'
  coverage-threshold:
    description: 'Minimum coverage threshold percentage'
    required: false
    default: '80'
  coverage-fail-under:
    description: 'Fail if coverage is under threshold'
    required: false
    default: 'false'
  coverage-exclude:
    description: 'Files/patterns to exclude from coverage'
    required: false
    default: 'tests/*,*/tests/*,*/venv/*,*/site-packages/*'
  # Test Execution
  parallel-tests:
    description: 'Run tests in parallel'
    required: false
    default: 'true'
  max-workers:
    description: 'Maximum number of test workers'
    required: false
    default: 'auto'
  test-timeout:
    description: 'Test timeout in seconds'
    required: false
    default: '300'
  # Reporting
  junit-xml:
    description: 'Generate JUnit XML report'
    required: false
    default: 'true'
  junit-xml-path:
    description: 'Path for JUnit XML report'
    required: false
    default: 'test-results.xml'
  html-report:
    description: 'Generate HTML test report'
    required: false
    default: 'false'
  html-report-path:
    description: 'Path for HTML report'
    required: false
    default: 'test-report.html'
  # Integration Testing
  run-integration-tests:
    description: 'Run integration tests'
    required: false
    default: 'false'
  integration-test-path:
    description: 'Path to integration tests'
    required: false
    default: 'tests/integration'
  integration-test-args:
    description: 'Additional integration test arguments'
    required: false
    default: ''
  # Performance Testing
  run-performance-tests:
    description: 'Run performance tests'
    required: false
    default: 'false'
  performance-test-path:
    description: 'Path to performance tests'
    required: false
    default: 'tests/performance'
  performance-test-args:
    description: 'Additional performance test arguments'
    required: false
    default: ''
  # Test Dependencies
  install-test-deps:
    description: 'Install test dependencies'
    required: false
    default: 'true'
  test-requirements-file:
    description: 'Test requirements file'
    required: false
    default: 'requirements-test.txt'
  # Environment
  test-env-vars:
    description: 'Environment variables for tests (JSON object)'
    required: false
    default: '{}'

outputs:
  test-result:
    description: 'Overall test result (success/failure)'
    value: ${{ steps.run-tests.outputs.result }}
  test-count:
    description: 'Number of tests executed'
    value: ${{ steps.parse-results.outputs.test-count }}
  test-passed:
    description: 'Number of tests passed'
    value: ${{ steps.parse-results.outputs.passed }}
  test-failed:
    description: 'Number of tests failed'
    value: ${{ steps.parse-results.outputs.failed }}
  test-skipped:
    description: 'Number of tests skipped'
    value: ${{ steps.parse-results.outputs.skipped }}
  coverage-percentage:
    description: 'Code coverage percentage'
    value: ${{ steps.coverage-report.outputs.percentage }}
  coverage-report-path:
    description: 'Path to coverage report'
    value: ${{ steps.coverage-report.outputs.report-path }}
  junit-xml-path:
    description: 'Path to JUnit XML report'
    value: ${{ steps.run-tests.outputs.junit-path }}
  duration:
    description: 'Test execution duration in seconds'
    value: ${{ steps.parse-results.outputs.duration }}

runs:
  using: 'composite'
  steps:
    - name: ðŸ§ª Setup Test Environment
      shell: bash
      working-directory: ${{ inputs.working-directory }}
      run: |
        echo "ðŸ§ª Setting up Python testing environment"
        echo "Framework: ${{ inputs.test-framework }}"
        echo "Test Path: ${{ inputs.test-path }}"
        echo "Coverage: ${{ inputs.collect-coverage }}"
        echo "Parallel: ${{ inputs.parallel-tests }}"
        
        # Create test results directory
        mkdir -p test-results
        mkdir -p coverage-reports
        
        # Set environment variables for tests
        if [ "${{ inputs.test-env-vars }}" != "{}" ]; then
          echo "Setting up test environment variables..."
          echo '${{ inputs.test-env-vars }}' | jq -r 'to_entries|map("\(.key)=\(.value|tostring)")|.[]' >> $GITHUB_ENV
        fi

    - name: ðŸ“¦ Install Test Dependencies
      if: inputs.install-test-deps == 'true'
      shell: bash
      working-directory: ${{ inputs.working-directory }}
      run: |
        echo "ðŸ“¦ Installing test dependencies..."
        
        # Install framework-specific dependencies
        case "${{ inputs.test-framework }}" in
          pytest)
            echo "Installing pytest and plugins..."
            pip install pytest pytest-cov pytest-xdist pytest-html pytest-json-report pytest-timeout
            if [ "${{ inputs.parallel-tests }}" = "true" ]; then
              pip install pytest-xdist
            fi
            ;;
          unittest)
            echo "Using built-in unittest framework"
            if [ "${{ inputs.collect-coverage }}" = "true" ]; then
              pip install coverage
            fi
            ;;
          nose2)
            echo "Installing nose2..."
            pip install nose2 nose2-cov
            ;;
          tox)
            echo "Installing tox..."
            pip install tox
            ;;
        esac
        
        # Install additional test requirements
        if [ -f "${{ inputs.test-requirements-file }}" ]; then
          echo "Installing from ${{ inputs.test-requirements-file }}"
          pip install -r "${{ inputs.test-requirements-file }}"
        fi
        
        # Install common testing utilities
        pip install coverage[toml] || true

    - name: ðŸ§ª Run Unit Tests
      id: run-tests
      shell: bash
      working-directory: ${{ inputs.working-directory }}
      run: |
        echo "ðŸ§ª Running unit tests with ${{ inputs.test-framework }}..."
        
        JUNIT_PATH="test-results/${{ inputs.junit-xml-path }}"
        TEST_RESULT="success"
        START_TIME=$(date +%s)
        
        # Prepare test command based on framework
        if [ -n "${{ inputs.test-command }}" ]; then
          # Custom test command
          TEST_CMD="${{ inputs.test-command }}"
        else
          case "${{ inputs.test-framework }}" in
            pytest)
              TEST_CMD="pytest ${{ inputs.test-path }}"
              
              # Add JUnit XML output
              if [ "${{ inputs.junit-xml }}" = "true" ]; then
                TEST_CMD="$TEST_CMD --junitxml=$JUNIT_PATH"
              fi
              
              # Add coverage
              if [ "${{ inputs.collect-coverage }}" = "true" ]; then
                COVERAGE_FORMATS="${{ inputs.coverage-format }}"
                for format in ${COVERAGE_FORMATS//,/ }; do
                  TEST_CMD="$TEST_CMD --cov=${{ inputs.coverage-source }} --cov-report=$format"
                done
                TEST_CMD="$TEST_CMD --cov-report=term-missing"
                
                if [ "${{ inputs.coverage-fail-under }}" = "true" ]; then
                  TEST_CMD="$TEST_CMD --cov-fail-under=${{ inputs.coverage-threshold }}"
                fi
              fi
              
              # Add parallel execution
              if [ "${{ inputs.parallel-tests }}" = "true" ]; then
                if [ "${{ inputs.max-workers }}" = "auto" ]; then
                  TEST_CMD="$TEST_CMD -n auto"
                else
                  TEST_CMD="$TEST_CMD -n ${{ inputs.max-workers }}"
                fi
              fi
              
              # Add timeout
              TEST_CMD="$TEST_CMD --timeout=${{ inputs.test-timeout }}"
              
              # Add HTML report
              if [ "${{ inputs.html-report }}" = "true" ]; then
                TEST_CMD="$TEST_CMD --html=test-results/${{ inputs.html-report-path }} --self-contained-html"
              fi
              
              # Add custom args
              TEST_CMD="$TEST_CMD ${{ inputs.test-args }}"
              ;;
              
            unittest)
              TEST_CMD="python -m unittest discover -s ${{ inputs.test-path }} -p '${{ inputs.test-pattern }}' -v"
              
              if [ "${{ inputs.collect-coverage }}" = "true" ]; then
                TEST_CMD="coverage run --source=${{ inputs.coverage-source }} -m unittest discover -s ${{ inputs.test-path }} -p '${{ inputs.test-pattern }}' -v"
              fi
              ;;
              
            nose2)
              TEST_CMD="nose2 --start-dir ${{ inputs.test-path }}"
              
              if [ "${{ inputs.collect-coverage }}" = "true" ]; then
                TEST_CMD="$TEST_CMD --with-coverage --coverage=${{ inputs.coverage-source }}"
              fi
              
              if [ "${{ inputs.junit-xml }}" = "true" ]; then
                TEST_CMD="$TEST_CMD --plugin nose2.plugins.junitxml --junit-xml"
              fi
              ;;
              
            tox)
              TEST_CMD="tox"
              ;;
          esac
        fi
        
        echo "Executing: $TEST_CMD"
        
        # Run tests with error handling
        if eval "$TEST_CMD"; then
          echo "âœ… Tests passed successfully"
          TEST_RESULT="success"
        else
          echo "âŒ Tests failed"
          TEST_RESULT="failure"
        fi
        
        END_TIME=$(date +%s)
        DURATION=$((END_TIME - START_TIME))
        
        echo "result=$TEST_RESULT" >> $GITHUB_OUTPUT
        echo "junit-path=$JUNIT_PATH" >> $GITHUB_OUTPUT
        echo "duration=$DURATION" >> $GITHUB_OUTPUT
        echo "Test execution took $DURATION seconds"

    - name: ðŸ§ª Run Integration Tests
      if: inputs.run-integration-tests == 'true'
      shell: bash
      working-directory: ${{ inputs.working-directory }}
      continue-on-error: true
      run: |
        echo "ðŸ§ª Running integration tests..."
        
        if [ -d "${{ inputs.integration-test-path }}" ]; then
          case "${{ inputs.test-framework }}" in
            pytest)
              pytest "${{ inputs.integration-test-path }}" \
                --junitxml=test-results/integration-test-results.xml \
                ${{ inputs.integration-test-args }}
              ;;
            unittest)
              python -m unittest discover -s "${{ inputs.integration-test-path }}" -v
              ;;
          esac
        else
          echo "âš ï¸ Integration test path not found: ${{ inputs.integration-test-path }}"
        fi

    - name: âš¡ Run Performance Tests
      if: inputs.run-performance-tests == 'true'
      shell: bash
      working-directory: ${{ inputs.working-directory }}
      continue-on-error: true
      run: |
        echo "âš¡ Running performance tests..."
        
        if [ -d "${{ inputs.performance-test-path }}" ]; then
          # Install performance testing tools
          pip install pytest-benchmark || true
          
          case "${{ inputs.test-framework }}" in
            pytest)
              pytest "${{ inputs.performance-test-path }}" \
                --benchmark-only \
                --benchmark-json=test-results/benchmark.json \
                ${{ inputs.performance-test-args }}
              ;;
            unittest)
              python -m unittest discover -s "${{ inputs.performance-test-path }}" -v
              ;;
          esac
        else
          echo "âš ï¸ Performance test path not found: ${{ inputs.performance-test-path }}"
        fi

    - name: ðŸ“Š Generate Coverage Report
      id: coverage-report
      if: inputs.collect-coverage == 'true'
      shell: bash
      working-directory: ${{ inputs.working-directory }}
      run: |
        echo "ðŸ“Š Generating coverage reports..."
        
        COVERAGE_PERCENTAGE=0
        COVERAGE_REPORT_PATH=""
        
        if [ "${{ inputs.test-framework }}" = "unittest" ]; then
          # Generate coverage reports for unittest
          COVERAGE_FORMATS="${{ inputs.coverage-format }}"
          for format in ${COVERAGE_FORMATS//,/ }; do
            case $format in
              xml)
                coverage xml -o coverage-reports/coverage.xml
                COVERAGE_REPORT_PATH="coverage-reports/coverage.xml"
                ;;
              html)
                coverage html -d coverage-reports/htmlcov
                ;;
              json)
                coverage json -o coverage-reports/coverage.json
                ;;
              term)
                coverage report
                ;;
            esac
          done
          
          # Extract coverage percentage
          COVERAGE_PERCENTAGE=$(coverage report --format=total 2>/dev/null || echo "0")
        else
          # For pytest, coverage files should already be generated
          if [ -f "coverage.xml" ]; then
            cp coverage.xml coverage-reports/
            COVERAGE_REPORT_PATH="coverage-reports/coverage.xml"
            
            # Extract coverage percentage from XML
            if command -v python3 >/dev/null 2>&1; then
              COVERAGE_PERCENTAGE=$(python3 -c "
import xml.etree.ElementTree as ET
try:
    tree = ET.parse('coverage.xml')
    root = tree.getroot()
    line_rate = float(root.attrib.get('line-rate', 0))
    print(int(line_rate * 100))
except:
    print(0)
" 2>/dev/null || echo "0")
            fi
          fi
          
          if [ -d "htmlcov" ]; then
            cp -r htmlcov coverage-reports/
          fi
        fi
        
        echo "percentage=$COVERAGE_PERCENTAGE" >> $GITHUB_OUTPUT
        echo "report-path=$COVERAGE_REPORT_PATH" >> $GITHUB_OUTPUT
        echo "ðŸ“Š Coverage: $COVERAGE_PERCENTAGE%"
        
        # Check coverage threshold
        if [ "${{ inputs.coverage-fail-under }}" = "true" ]; then
          if [ "$COVERAGE_PERCENTAGE" -lt "${{ inputs.coverage-threshold }}" ]; then
            echo "âŒ Coverage $COVERAGE_PERCENTAGE% is below threshold ${{ inputs.coverage-threshold }}%"
            exit 1
          else
            echo "âœ… Coverage $COVERAGE_PERCENTAGE% meets threshold ${{ inputs.coverage-threshold }}%"
          fi
        fi

    - name: ðŸ“ˆ Parse Test Results
      id: parse-results
      shell: bash
      working-directory: ${{ inputs.working-directory }}
      run: |
        echo "ðŸ“ˆ Parsing test results..."
        
        TEST_COUNT=0
        PASSED=0
        FAILED=0
        SKIPPED=0
        DURATION=0
        
        # Parse JUnit XML if available
        JUNIT_FILE="test-results/${{ inputs.junit-xml-path }}"
        if [ -f "$JUNIT_FILE" ] && command -v python3 >/dev/null 2>&1; then
          RESULTS=$(python3 -c "
import xml.etree.ElementTree as ET
import sys
try:
    tree = ET.parse('$JUNIT_FILE')
    root = tree.getroot()
    
    # Handle different JUnit formats
    if root.tag == 'testsuites':
        testsuite = root.find('testsuite')
        if testsuite is not None:
            tests = int(testsuite.get('tests', 0))
            failures = int(testsuite.get('failures', 0))
            errors = int(testsuite.get('errors', 0))
            skipped = int(testsuite.get('skipped', 0))
            time = float(testsuite.get('time', 0))
        else:
            tests = int(root.get('tests', 0))
            failures = int(root.get('failures', 0))
            errors = int(root.get('errors', 0))
            skipped = int(root.get('skipped', 0))
            time = float(root.get('time', 0))
    else:
        tests = int(root.get('tests', 0))
        failures = int(root.get('failures', 0))
        errors = int(root.get('errors', 0))
        skipped = int(root.get('skipped', 0))
        time = float(root.get('time', 0))
    
    passed = tests - failures - errors - skipped
    print(f'{tests},{passed},{failures + errors},{skipped},{int(time)}')
except Exception as e:
    print('0,0,0,0,0')
" 2>/dev/null || echo "0,0,0,0,0")
          
          IFS=',' read -r TEST_COUNT PASSED FAILED SKIPPED DURATION <<< "$RESULTS"
        fi
        
        echo "test-count=$TEST_COUNT" >> $GITHUB_OUTPUT
        echo "passed=$PASSED" >> $GITHUB_OUTPUT
        echo "failed=$FAILED" >> $GITHUB_OUTPUT
        echo "skipped=$SKIPPED" >> $GITHUB_OUTPUT
        echo "duration=$DURATION" >> $GITHUB_OUTPUT
        
        echo "ðŸ“ˆ Test Summary:"
        echo "  â€¢ Total Tests: $TEST_COUNT"
        echo "  â€¢ Passed: $PASSED"
        echo "  â€¢ Failed: $FAILED"
        echo "  â€¢ Skipped: $SKIPPED"
        echo "  â€¢ Duration: ${DURATION}s"

    - name: ðŸ“¤ Upload Test Artifacts
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: python-test-results
        path: |
          ${{ inputs.working-directory }}/test-results/
          ${{ inputs.working-directory }}/coverage-reports/
        retention-days: 30

branding:
  icon: 'check-circle'
  color: 'green'